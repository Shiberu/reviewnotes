multiprocessors (MIMD)
  cache
    when any processor has cache miss or writes, notify other processors via interconnection network
      if only reading, many processors can have copies
      if a processor writes, invalidate all other copies
    false sharing
      different processors read & write data in the same cache block
  threads
    thread of execution: smallest unit of processing scheduled by operating system
    single processor, multithreading by time-division multiplexing
      processor switched between different threads
      context switching fast enough
    multiprocessor (multicore), threads run at same time on different processors
  openmp: API for multi-threaded, shared memory parallelism
    data race: 2 memory accesses from different threads to same location
      results can vary
      avoid by synchronizing writing and reading
    pragma: a mechanism C provides for language extensions
      compilers that don't recognize a pragma are supposed to ignore them
    fork/join parallelism
      master thread -> [enter parallel code] fork worker threads -> join [end of parallel]
    simple parallelization of for-loop
      code
        #pragma omp parallel for
        for (i = 0; i < max; i++) blablabla;
      must have canonical shape, no break / return / exit / goto inside loop
      all variables declared outside for loop are shared by default
    set threads via
      omp_set_num_threads(NUM_THREADS);
    invoking parallel threads
      #pragma omp parallel
      {
        int ID = omp_get_thread_num();
      }
    reduction (operator:var)
      declare variable inside for-loop private by default
    omp_get_wtime()
  strong scaling: problem size fixed (harder). speed-up
  weak scaling: problem size proportional to increase in number of processors. scale-up

hardware
  electro-mechanical replays -> electronic vacuum tubes -> transistors
  synchronous digital systems
    example: MIPS
    synchronous: all operations coordinated by a central clock
    digital: represent all values by 2 discret values; high/low voltage for 1/0
  switches
    close switch, open switch, AND (series connection), OR (parallel connection)
  transistors
    high voltage (V_dd) represents 1 / true
    low voltage (0V / ground) represents 0 / false
    threshold voltage (V_th) decide if 0 or 1
  CMOS transistor
    MOS: Metal-Oxide on Semiconductor
    C: complementary. use pairs of normally-open and normally-closed switches
    source --|gate|--> drain
    n-channel transistor: gate (_|_) open when voltage at Gate is low; close when voltage(gate) > voltage(threshold)
      pass weak 0's, strong 1's
    p-channel transistor: gate (_b_) close when voltage at Gate is low; open when voltage(gate) > voltage(threshold)
      pass weak 1's, strong 0's
    use pairs of N-type and P-type to get strong values
    make sure there's always a path to V_dd or ground
    never create a path from v_dd to ground
    inverter / not gate
    NAND gate
  type of Synchronous Digital System circuits
    combinational logic (CL) circuits: no history
    sequential logic (SL) circults: store information. aka state element
  symbols:
    buffer: triangle. amplify signal
    NOT: triangle + dot
    AND, NAND
    OR, NOR
    multiplexer
  boolean algebra
    OR = +, AND = *, NOT = hat
  state elements
    usage: place to store values for some amount of time; control flow of information between combinational logic blocks
  register
    n instances of a (D-type) "Flip-Flop"
  hardware timing
    setup time: when the input must be stable before the edge of the CLK
    hold time: when the input must be stable after the edge of the CLK
    "CLK-to-Q" delay: how long it takes the output to change, measured from the edge of the CLK
  max delay = setup time + CLK-to-Q delay + CL delay
  logism
    blue wires: value unknown
    gray wires: not connected to anything
    bright green: 1
    dark green: 0
  asserted (enabled, 1) / deasserted (0)
  multiplexer: c = not(s)a + sb
  adder / subtractor: si = XOR(ai, bi, ci), ci+1 = MAJ(ai, bi, ci) = aibi + aici + bici
  critical path: path through CL that is worst case
  steps to design a processor:
    analyze instruction set -> datapath requirements
      general steps of data path
        IF: instruction fetch
        ID: instruction decode, read registers
        EX: execution. mem-ref calculate address / arith-log perform operation
        Mem: read data from memory / write data to memory
        WB: write data back to register
    select set of datapath components & establish clock methodology
      components: combinational elements, state elements + clocking methodology, building blocks
      storage element
        memory: data-in + clk + write-enable + address + data-out
        register: data-in + clk + data-out + write-enable
        register file: RW + RA + RB + write-enable + clk + busW + busA + busB
    assemble datapath meeting the requirements
    analyze implementation of each instruction to determine setting of control points that effects the register transfer
    assemble the control logic
      formulate logic equations & design circuits
  register transfer language
    example: (ADDU) R[rd] <- R[rs] + R[rt]; PC <- PC + 4
  instruction level parallelism (ILP)
    pipelining
      does not help latency of single task, but throughput of entire workload
      multiple tasks operating simultaneously using different resources
      potential speedup = number of pipeline stages
      time to fill pipeline and time to drain it reduces speedup
      speedup limited by slowest pipeline stage
    RISC design principles
      fixed instruction length
      simplified addressing modes
      fewer and simpler instructions
      simplified memory access
      let the compiler do it
    pipelined datapath
      add registers between stages to hold information produced in previous cycle
    hazards: situations that prevent starting the next logical instruction in the next clock cycle
      structural hazards
        requried resource is busy
        single memory
          pipeline bubble: instruction fetch has to stall for load/store instruction
          solution: provide separate instruction / data memories
        register
          access very fast, so write during first half and read during second half
          or build regfile with independent read and write ports
      data hazard
        need to wait for previous instruction to complete its data read/write
        solution: forwarding / bypassing
          use result when it is computed. requires extra connections in the datapath
          forwarding path: [rs, rt] --> [forwarding unit (-> multiplexer -> ALU)] <-- [EX/MEM/WB register]
        load-use data hazard
          cannot always avoid stalls -> no-op
      control hazard (BEQ / BNE)
        deciding on control action depends on previous instructiojn 
        simple solution: stall on every branch until new PC value (2 stalls per branch)
        optimization #1: insert special branch comparator in stage 2, set new PC as soon as instruction decoded (1 stalls per branch)
        optimization #2: predict outcome (simplest to predict NOT taken)
        optimization #3: branch-delay slot (always execute instruction after branch. used by MIPS)
          put no-op in the branch slot or place some instruction preceding the branch in the slot
        dynamic branch prediction
          branch prediction buffer / branch history table
          indexed by recent branch instruction address
          stores outcome (taken / not taken)
          update if wrong
          1-bit predictor shortcoming: inner / outer branch
          use 2-bit predictor: only change prediction on two successive misprediction
      sometimes can reorder code to avoid stalls
  greater instruction level parallelism
    deeper pipeline => less work per stage => shorter clock cycle
    multiple issue superscalar
      replicate pipeline stages => multiple pipelines
      multiple instructions per clock cycle
      IPC: instructions per cycle
      static multiple issue
        compiler group instructions that can be issued in a single cycle
          determined by pipeline resource required
        packages them into issue slots
        VLIW: very long instruction word
        issue
          compiler must remove some/all hazards
          no dependencies with a packet
          possibly some dependencies between packets, varies between ISAs, compiler must know
          pad with noop if necessary
        MIPS + static dual issue
          1 ALU/branch + 1 load/store
          pad unused instruction with noop
          datapath:
            1 extra 32-bit instruction, sign ext, write port, reg read ports, ALU
          hazards
            EX data hazard (can be avoided by forwarding in single-issue)
              split into two packets, effectively a stall
            load-use latency
              still one cycle latency, but now two instructions
        loop unrolling
          replicate loop body to expose more parallelism, reduces loop control overhead
          register renaming
            use different registers per replication
            avoid loop-carried anti-dependencies
      dynamic multiple issue
        CPU examines instruction stream and chooses instructions to issue each cycle
          avoid structural and data hazards
        compiler can help by reordering instructions
        CPU resolves hazards using advanced techniques at runtime

